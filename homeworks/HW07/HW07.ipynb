{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-17T20:04:22.043021Z",
     "start_time": "2026-01-17T19:37:42.059119400Z"
    }
   },
   "source": [
    "# ============================ HW07 – полный код ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    "    adjusted_rand_score as ARI,\n",
    ")\n",
    "import json, os\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Создаём только нужные подпапки рядом с ноутбуком\n",
    "# ------------------------------------------------------------------\n",
    "ARTIFACTS = \"artifacts\"\n",
    "os.makedirs(f\"{ARTIFACTS}/labels\", exist_ok=True)\n",
    "os.makedirs(f\"{ARTIFACTS}/figures\", exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Пути к датасетам (папка data уже существует)\n",
    "# ------------------------------------------------------------------\n",
    "DATASETS = {\n",
    "    \"ds1\": \"data/S07-hw-dataset-01.csv\",\n",
    "    \"ds2\": \"data/S07-hw-dataset-02.csv\",\n",
    "    \"ds3\": \"data/S07-hw-dataset-03.csv\",\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Утилиты\n",
    "# ------------------------------------------------------------------\n",
    "def load_and_preprocess(path: str):\n",
    "    \"\"\"Возвращает (X_scaled, sample_id).\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.drop(columns=[\"sample_id\"])\n",
    "    X = SimpleImputer(strategy=\"mean\").fit_transform(X)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return X, df[\"sample_id\"].values\n",
    "\n",
    "\n",
    "def cluster_metrics(X, labels):\n",
    "    \"\"\"Внутренние метрики кластеризации.\"\"\"\n",
    "    uniq = np.unique(labels)\n",
    "    if len(uniq) < 2:\n",
    "        return {\"silhouette\": None, \"davies_bouldin\": None, \"calinski_harabasz\": None}\n",
    "    return {\n",
    "        \"silhouette\": float(silhouette_score(X, labels)),\n",
    "        \"davies_bouldin\": float(davies_bouldin_score(X, labels)),\n",
    "        \"calinski_harabasz\": float(calinski_harabasz_score(X, labels)),\n",
    "    }\n",
    "\n",
    "\n",
    "def save_labels(sample_id, labels, suffix):\n",
    "    pd.DataFrame({\"sample_id\": sample_id, \"cluster_label\": labels})\\\n",
    "      .to_csv(f\"{ARTIFACTS}/labels/labels_{suffix}.csv\", index=False)\n",
    "\n",
    "\n",
    "def save_fig(name):\n",
    "    plt.savefig(f\"{ARTIFACTS}/figures/{name}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Загрузка данных\n",
    "# ------------------------------------------------------------------\n",
    "data = {}\n",
    "for name, path in DATASETS.items():\n",
    "    X, sid = load_and_preprocess(path)\n",
    "    data[name] = {\"X\": X, \"sample_id\": sid}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. KMeans (k = 2..20)\n",
    "# ------------------------------------------------------------------\n",
    "kmeans_res = {}\n",
    "for name in data:\n",
    "    X = data[name][\"X\"]\n",
    "    res = []\n",
    "    for k in range(2, 21):\n",
    "        labels = KMeans(k, n_init=10, random_state=42).fit_predict(X)\n",
    "        res.append({\"k\": k, \"labels\": labels, \"metrics\": cluster_metrics(X, labels)})\n",
    "    best = max(res, key=lambda x: x[\"metrics\"][\"silhouette\"])\n",
    "    kmeans_res[name] = best\n",
    "    save_labels(data[name][\"sample_id\"], best[\"labels\"], f\"{name}_kmeans\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. DBSCAN (eps 0.1-1.0, min_samples 5,8,12)\n",
    "# ------------------------------------------------------------------\n",
    "dbscan_res = {}\n",
    "for name in data:\n",
    "    X = data[name][\"X\"]\n",
    "    res = []\n",
    "    for eps in np.arange(0.1, 1.1, 0.1):\n",
    "        for ms in [5, 8, 12]:\n",
    "            labels = DBSCAN(eps=eps, min_samples=ms).fit_predict(X)\n",
    "            mask = labels != -1\n",
    "            if mask.sum() > 0 and len(np.unique(labels[mask])) >= 2:\n",
    "                res.append({\n",
    "                    \"eps\": eps,\n",
    "                    \"min_samples\": ms,\n",
    "                    \"labels\": labels,\n",
    "                    \"noise_frac\": 1 - mask.mean(),\n",
    "                    \"metrics\": cluster_metrics(X[mask], labels[mask]),\n",
    "                })\n",
    "    if res:\n",
    "        best = max(res, key=lambda x: x[\"metrics\"][\"silhouette\"])\n",
    "        dbscan_res[name] = best\n",
    "        save_labels(data[name][\"sample_id\"], best[\"labels\"], f\"{name}_dbscan\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7. Agglomerative (k = 2..10, linkage ward/complete/average)\n",
    "# ------------------------------------------------------------------\n",
    "agg_res = {}\n",
    "for name in data:\n",
    "    X = data[name][\"X\"]\n",
    "    res = []\n",
    "    for k in range(2, 11):\n",
    "        for link in [\"ward\", \"complete\", \"average\"]:\n",
    "            labels = AgglomerativeClustering(n_clusters=k, linkage=link).fit_predict(X)\n",
    "            res.append({\"k\": k, \"linkage\": link, \"labels\": labels, \"metrics\": cluster_metrics(X, labels)})\n",
    "    best = max(res, key=lambda x: x[\"metrics\"][\"silhouette\"])\n",
    "    agg_res[name] = best\n",
    "    save_labels(data[name][\"sample_id\"], best[\"labels\"], f\"{name}_agg\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8. PCA 2D-визуализации\n",
    "# ------------------------------------------------------------------\n",
    "for name in data:\n",
    "    X = data[name][\"X\"]\n",
    "    X2 = PCA(n_components=2, random_state=42).fit_transform(X)\n",
    "    for algo, res in [\n",
    "        (\"kmeans\", kmeans_res[name]),\n",
    "        (\"dbscan\", dbscan_res.get(name, {})),\n",
    "        (\"agg\", agg_res[name]),\n",
    "    ]:\n",
    "        if not res:\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.scatter(X2[:, 0], X2[:, 1], c=res[\"labels\"], s=15, cmap=\"tab10\")\n",
    "        plt.title(f\"{name.upper()} – {algo.upper()} – PCA(2D)\")\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        save_fig(f\"{name}_{algo}_pca\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 9. Устойчивость KMeans (ds1, 5 seed)\n",
    "# ------------------------------------------------------------------\n",
    "X = data[\"ds1\"][\"X\"]\n",
    "labels_list = [\n",
    "    KMeans(n_clusters=kmeans_res[\"ds1\"][\"k\"], n_init=10, random_state=seed).fit_predict(X)\n",
    "    for seed in range(5)\n",
    "]\n",
    "ari_vals = [ARI(labels_list[i], labels_list[j]) for i in range(5) for j in range(i + 1, 5)]\n",
    "stability = {\"mean_ari\": float(np.mean(ari_vals)), \"std_ari\": float(np.std(ari_vals))}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 10. Сводные JSON-файлы\n",
    "# ------------------------------------------------------------------\n",
    "metrics_summary = {\n",
    "    name: {\n",
    "        \"kmeans\": kmeans_res[name][\"metrics\"],\n",
    "        \"dbscan\": dbscan_res.get(name, {}).get(\"metrics\", {}),\n",
    "        \"agg\": agg_res[name][\"metrics\"],\n",
    "    }\n",
    "    for name in data\n",
    "}\n",
    "\n",
    "best_configs = {\n",
    "    name: {\n",
    "        \"kmeans\": {\"k\": kmeans_res[name][\"k\"]},\n",
    "        \"dbscan\": {\n",
    "            k: v\n",
    "            for k, v in dbscan_res.get(name, {}).items()\n",
    "            if k in [\"eps\", \"min_samples\", \"noise_frac\"]\n",
    "        },\n",
    "        \"agg\": {\"k\": agg_res[name][\"k\"], \"linkage\": agg_res[name][\"linkage\"]},\n",
    "    }\n",
    "    for name in data\n",
    "}\n",
    "\n",
    "with open(f\"{ARTIFACTS}/metrics_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "\n",
    "with open(f\"{ARTIFACTS}/best_configs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_configs, f, indent=2)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 11. report.md (UTF-8)\n",
    "# ------------------------------------------------------------------\n",
    "with open(\"report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\n",
    "        f\"\"\"# HW07 Report\n",
    "\n",
    "## 1. Datasets\n",
    "ds1, ds2, ds3 – 500×5, numeric, no missing values\n",
    "\n",
    "## 2. Protocol\n",
    "- Pre-processing: StandardScaler\n",
    "- KMeans: k = 2..20\n",
    "- DBSCAN: eps = 0.1..1.0, min_samples = 5, 8, 12\n",
    "- Agglomerative: k = 2..10, linkage ∈ {{\"ward\", \"complete\", \"average\"}}\n",
    "\n",
    "## 3. Models\n",
    "See `best_configs.json`\n",
    "\n",
    "## 4. Results\n",
    "See `metrics_summary.json`\n",
    "\n",
    "## 5. Analysis\n",
    "KMeans stability (ds1): mean ARI = {stability['mean_ari']:.3f} ± {stability['std_ari']:.3f}\n",
    "DBSCAN captures non-spherical clusters; Agglomerative competitive on spherical data\n",
    "\n",
    "## 6. Conclusion\n",
    "Feature scaling is essential for KMeans; DBSCAN is sensitive to eps/min_samples; PCA-2D is sufficient for visualization\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "print(\"Готово – все артефакты сохранены в папке artifacts и report.md\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово – все артефакты сохранены в папке artifacts и report.md\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
